{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1gmpJwdmNDqJP2g13RF9Fkph507UvTmpC",
      "authorship_tag": "ABX9TyPT/ABTlCXlU6cJCn0JnUfe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnthonyPerezGit/DatingApp-V1.0/blob/master/Roof_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i67NLF5CCqnX",
        "outputId": "953cee1f-11c3-44ef-cbd6-191484c4d3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/22.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/22.2 MB\u001b[0m \u001b[31m188.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/22.2 MB\u001b[0m \u001b[31m192.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.1/22.2 MB\u001b[0m \u001b[31m179.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m170.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m170.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Libraries Installed!\n",
            "üì¶ Downloading SpaceNet Vegas Dataset...\n",
            "--2025-04-21 02:58:47--  https://spacenet-dataset.s3.amazonaws.com/spacenet/SN2_buildings/tarballs/SN2_buildings_train_AOI_2_Vegas.tar.gz\n",
            "Resolving spacenet-dataset.s3.amazonaws.com (spacenet-dataset.s3.amazonaws.com)... 3.5.0.10, 3.5.25.33, 16.15.217.244, ...\n",
            "Connecting to spacenet-dataset.s3.amazonaws.com (spacenet-dataset.s3.amazonaws.com)|3.5.0.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25640007327 (24G) [application/x-tar]\n",
            "Saving to: ‚ÄòSN2_buildings_train_AOI_2_Vegas.tar.gz‚Äô\n",
            "\n",
            "SN2_buildings_train 100%[===================>]  23.88G  47.5MB/s    in 8m 21s  \n",
            "\n",
            "2025-04-21 03:07:09 (48.8 MB/s) - ‚ÄòSN2_buildings_train_AOI_2_Vegas.tar.gz‚Äô saved [25640007327/25640007327]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# üõ∞Ô∏è Real Roof Detection AI Training Notebook (Fully Fixed Version)\n",
        "# Author: Your AI Partner üöÄ\n",
        "\n",
        "# ============================================\n",
        "# STEP 0: Install Core Dependencies\n",
        "\n",
        "!pip install -U \"huggingface_hub[cli]\" rasterio shapely geopandas tqdm --quiet\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import rasterio\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import shape\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"‚úÖ Libraries Installed!\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 1: Download SpaceNet Vegas Dataset (Direct Link)\n",
        "\n",
        "print(\"üì¶ Downloading SpaceNet Vegas Dataset...\")\n",
        "\n",
        "!wget -O SN2_buildings_train_AOI_2_Vegas.tar.gz \"https://spacenet-dataset.s3.amazonaws.com/spacenet/SN2_buildings/tarballs/SN2_buildings_train_AOI_2_Vegas.tar.gz\"\n",
        "!mkdir -p spacenet_vegas\n",
        "!tar -xzf SN2_buildings_train_AOI_2_Vegas.tar.gz -C spacenet_vegas\n",
        "\n",
        "print(\"‚úÖ SpaceNet Vegas dataset downloaded and extracted!\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 2: Generate Images and Masks\n",
        "\n",
        "IMAGE_DIR = \"/content/spacenet_vegas/AOI_2_Vegas/PS-RGB/\"\n",
        "LABEL_DIR = \"/content/spacenet_vegas/AOI_2_Vegas/geojson/buildings/train/\"\n",
        "OUTPUT_DIR = \"/content/real_roof_dataset/\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR + \"images\", exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR + \"masks\", exist_ok=True)\n",
        "\n",
        "# Helper to rasterize polygons into mask\n",
        "def rasterize_shapes(shapes, out_shape):\n",
        "    return rasterio.features.rasterize(\n",
        "        ((geom, 1) for geom in shapes),\n",
        "        out_shape=out_shape,\n",
        "        fill=0,\n",
        "        dtype='uint8'\n",
        "    )\n",
        "\n",
        "print(\"üõ† Generating masks from polygons...\")\n",
        "\n",
        "image_paths = sorted(glob.glob(IMAGE_DIR + \"*.tif\"))\n",
        "for img_path in tqdm(image_paths[:100]):  # Limit to 100 samples\n",
        "    base = os.path.basename(img_path).replace(\".tif\", \"\")\n",
        "    mask_path = LABEL_DIR + base + \".geojson\"\n",
        "\n",
        "    if not os.path.exists(mask_path):\n",
        "        continue\n",
        "\n",
        "    # Load image\n",
        "    img = rasterio.open(img_path).read([1, 2, 3])\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Load polygons\n",
        "    gdf = gpd.read_file(mask_path)\n",
        "    polygons = gdf.geometry.values\n",
        "\n",
        "    # Create mask\n",
        "    mask = rasterize_shapes(polygons, out_shape=img.shape[:2])\n",
        "\n",
        "    # Save JPG + PNG\n",
        "    img_save_path = os.path.join(OUTPUT_DIR, \"images\", base + \".jpg\")\n",
        "    mask_save_path = os.path.join(OUTPUT_DIR, \"masks\", base + \"_mask.png\")\n",
        "\n",
        "    cv2.imwrite(img_save_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "    cv2.imwrite(mask_save_path, mask * 255)\n",
        "\n",
        "print(\"‚úÖ Image and mask generation complete!\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 3: Load Dataset\n",
        "\n",
        "IMG_SIZE = 128\n",
        "\n",
        "print(\"üõ† Loading dataset...\")\n",
        "\n",
        "def load_data(img_dir, mask_dir, img_size=IMG_SIZE):\n",
        "    img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")))\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    for img_path in img_paths:\n",
        "        base = os.path.basename(img_path).replace('.jpg', '')\n",
        "        mask_path = os.path.join(mask_dir, base + '_mask.png')\n",
        "\n",
        "        if not os.path.exists(mask_path):\n",
        "            continue\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, (img_size, img_size))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        images.append(img / 255.0)\n",
        "\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, (img_size, img_size))\n",
        "        mask = np.expand_dims(mask, axis=-1) / 255.0\n",
        "        masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "images, masks = load_data(OUTPUT_DIR + \"images\", OUTPUT_DIR + \"masks\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(images)} real roof image-mask pairs!\")\n",
        "\n",
        "# ============================================\n",
        "# STEP 4: Train U-Net Model\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"‚úÖ Training set: {len(X_train)} samples, Validation set: {len(X_val)} samples\")\n",
        "\n",
        "# Build U-Net\n",
        "\n",
        "def unet_model(input_size=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    inputs = tf.keras.Input(input_size)\n",
        "\n",
        "    c1 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
        "    c1 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(p1)\n",
        "    c2 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(p2)\n",
        "    c3 = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(p3)\n",
        "    c4 = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(c4)\n",
        "\n",
        "    u5 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding=\"same\")(c4)\n",
        "    u5 = layers.concatenate([u5, c3])\n",
        "    c5 = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(u5)\n",
        "    c5 = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(c5)\n",
        "\n",
        "    u6 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding=\"same\")(c5)\n",
        "    u6 = layers.concatenate([u6, c2])\n",
        "    c6 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(u6)\n",
        "    c6 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(c6)\n",
        "\n",
        "    u7 = layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding=\"same\")(c6)\n",
        "    u7 = layers.concatenate([u7, c1])\n",
        "    c7 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(u7)\n",
        "    c7 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(c7)\n",
        "\n",
        "    outputs = layers.Conv2D(1, (1, 1), activation=\"sigmoid\")(c7)\n",
        "\n",
        "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "    return model\n",
        "\n",
        "print(\"üõ† Building U-Net model...\")\n",
        "\n",
        "model = unet_model()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# ============================================\n",
        "# STEP 5: Train Model\n",
        "\n",
        "print(\"üèãÔ∏è Training model...\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    batch_size=4,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# STEP 6: Plot Training Curves\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curves')\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# STEP 7: Save Model\n",
        "\n",
        "model.save('roof_spacenet_model.h5')\n",
        "print(\"‚úÖ Model saved as roof_spacenet_model.h5!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYtK2mGKCG5E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}